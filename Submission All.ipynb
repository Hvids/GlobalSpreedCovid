{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, BatchNormalization\n",
    "from datetime import datetime, timedelta,date\n",
    "from mapper import mapper_ru,mapper_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_time_series = './data/COVID-19_plus_Russia/csse_covid_19_data/csse_covid_19_time_series/'\n",
    "recovered_global_name = 'time_series_covid19_recovered_global.csv'\n",
    "recovered_ru_name = 'time_series_covid19_confirmed_RU.csv'\n",
    "deaths_global_name = 'time_series_covid19_deaths_global.csv'\n",
    "deaths_ru_name = 'time_series_covid19_deaths_RU.csv'\n",
    "path = './data/'\n",
    "countries_data_name = 'countries.csv'\n",
    "russia_regions_name = 'russia_regions.csv'\n",
    "sample_submission = 'sample_submission.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    def __init__(self, model,last_date,columns_df):\n",
    "        self.columns_df = columns_df\n",
    "        self.model = model\n",
    "        self.last_date = last_date\n",
    "        self.predion_date = self.__make_date_predition()\n",
    "        self.size_prediction = len(self.predion_date)\n",
    "        \n",
    "    def predict(self,dataset,n_features,n_steps):\n",
    "        self.dateset = dataset\n",
    "        matrix_prediction = self.__make_prediction_matrix(n_features,n_steps)\n",
    "        return self.__make_dataframe_prediction(matrix_prediction)\n",
    "    \n",
    "    def __make_dataframe_prediction(self,matrix_prediction):\n",
    "        df = pd.DataFrame(matrix_prediction,columns=self.columns_df)\n",
    "        df['date'] = self.predion_date\n",
    "        return df\n",
    "    def __make_prediction_matrix(self,n_features,n_steps):\n",
    "        dataset = self.dateset\n",
    "        model = self.model\n",
    "        count = self.size_prediction\n",
    "        \n",
    "        x = np.array([dataset[-n_steps:,:]])\n",
    "        x_i = dataset[-n_steps+1:,:]\n",
    "        result = []\n",
    "        for _ in range(count):\n",
    "            y_pred = model.predict(x)[0]\n",
    "    #         y_pred_ = list(np.sign(y_pred)*(y_pred))\n",
    "            y_pred_ = list((y_pred))\n",
    "            result.append(y_pred_)\n",
    "            y_pred = y_pred.reshape(1,n_features)\n",
    "            x = np.vstack((x_i,y_pred))\n",
    "            x_i = x[-n_steps+1:,:]\n",
    "            x = np.array([x])\n",
    "        return result\n",
    "        \n",
    "    def __make_date_predition(self):\n",
    "        last_date = self.last_date\n",
    "        last_date += timedelta(days=1)\n",
    "        res_date = []\n",
    "        while last_date< date(2021,1,1):\n",
    "            res_date.append(last_date)\n",
    "            last_date += timedelta(days=1)\n",
    "        return res_date\n",
    "        \n",
    "        \n",
    "    \n",
    "class Submission:\n",
    "    def __init__(self,columns_df,date_prediction):\n",
    "        self.columns_df = columns_df\n",
    "        self.date_prediction = date_prediction\n",
    "        \n",
    "    def submit(self,rec_df,deaths_df,mapper):\n",
    "        df_res = pd.DataFrame({}) \n",
    "        for column in self.columns_df:\n",
    "            df_res_ = pd.DataFrame({}) \n",
    "            df_res_['date'] = self.date_prediction\n",
    "            df_res_['region'] = [column]*len(self.date_prediction)\n",
    "            df_res_['prediction_confirmed'] = rec_df[column]\n",
    "            df_res_['prediction_deaths'] = deaths_df[column]\n",
    "            \n",
    "            df_res = pd.concat([df_res,df_res_],axis=0)\n",
    "        df_res.index=np.arange(df_res.shape[0])\n",
    "        df_res.region = df_res.region.map(mapper)\n",
    "        return df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps):\n",
    "    n = sequences.shape[0]\n",
    "    X, y = list(), list()\n",
    "    for i in range(n):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps\n",
    "    # check if we are beyond the dataset\n",
    "        if end_ix > n-1:\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_model_simple(n_steps,n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_features, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    # model.add(RepeatVector(n_features))\n",
    "    model.add(LSTM(n_features, activation='relu'))\n",
    "    \n",
    "    # model.add(LSTM(n_features, activation='relu'))\n",
    "\n",
    "    # model.add(Dense(n_features*4,activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features*3,activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features*2,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_recovered_ru(n_steps,n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_features, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "  \n",
    "    model.add(LSTM(n_features, activation='relu'))\n",
    "    model.add(Dense(n_features*3,activation='relu'))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features*2,activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features,activation='relu'))\n",
    "    # model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_features))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "def build_model_deaths_ru(n_steps,n_features):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_features*2, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "    model.add(LSTM(n_features, activation='relu'))\n",
    "    model.add(Dense(n_features*2,activation='relu'))\n",
    "\n",
    "    model.add(Dense(n_features))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_ru(df):\n",
    "    df = pd.concat([df.iloc[:,6:7], df.iloc[:,11:]],axis=1)\n",
    "    return df\n",
    "def get_dataset_ru(df):\n",
    "    solution_df = pd.DataFrame({'name':df['Province_State'].values})\n",
    "    date = df.columns.values[1:]\n",
    "    solution = pd.concat([solution_df,df[date]],axis=1)\n",
    "    dataset = solution[date].values.T \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_ru = pd.read_csv(path_time_series+ recovered_ru_name)\n",
    "deaths_ru = pd.read_csv(path_time_series + deaths_ru_name)\n",
    "recovered_ru = pre_ru(recovered_ru)\n",
    "deaths_ru = pre_ru(deaths_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOVERED RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hvidsmen/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_rec = get_dataset_ru(recovered_ru) \n",
    "\n",
    "n_steps = 3\n",
    "n_features = dataset_rec.shape[1]\n",
    "model_rec = build_model_simple(n_steps,n_features)\n",
    "X_rec,y_rec = split_sequences(dataset_rec,n_steps)\n",
    "hisoty_rec = model_rec.fit(X_rec, y_rec, epochs=EPOCHS, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEATHS RU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_deaths = get_dataset_ru(deaths_ru) \n",
    "n_steps = 3\n",
    "n_features =dataset_deaths.shape[1]\n",
    "model_deaths = build_model_simple(n_steps,n_features)\n",
    "\n",
    "\n",
    "X_deaths,y_deaths = split_sequences(dataset_deaths,n_steps)\n",
    "hisoty_deaths = model_deaths.fit(X_deaths, y_deaths, epochs=EPOCHS, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_ru = recovered_ru['Province_State']\n",
    "last_date = pd.to_datetime(recovered_ru.columns.values[-1:]).date[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictioner_rec = Prediction(model_rec,last_date,columns_ru)\n",
    "df_pred_rec = predictioner_rec.predict(dataset_rec,n_features,n_steps)\n",
    "predictioner_deaths = Prediction(model_deaths,last_date,columns_ru)\n",
    "df_pred_deaths = predictioner_deaths.predict(dataset_deaths,n_features,n_steps)\n",
    "submissioner = Submission(columns_ru,predictioner_rec.predion_date)\n",
    "subm_ru = submissioner.submit(df_pred_rec,df_pred_deaths,mapper_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_global(df, columns_uniq):\n",
    "    df = pd.concat([df.iloc[:,1:2], df.iloc[:,4:]],axis=1)\n",
    "#     print(df)\n",
    "    for country in columns_uniq:\n",
    "#         print(country)\n",
    "        idx = df['Country/Region']==country\n",
    "        idx_n =df['Country/Region']!=country\n",
    "        df_ = pd.DataFrame((df[idx].iloc[:,1:].sum(axis=0))).T\n",
    "        df_['Country/Region'] = country\n",
    "        cols = df_.columns.tolist()\n",
    "        cols = cols[-1:] + cols[:-1]\n",
    "        df_ = df_[cols]\n",
    "        df = df[idx_n]\n",
    "        df = pd.concat([df,df_],axis=0)\n",
    "    df.index = np.arange(df.shape[0])\n",
    "    return df\n",
    "\n",
    "def get_dataset_global(df):\n",
    "    solution_df = pd.DataFrame({'name':df['Country/Region'].values})\n",
    "    date = df.columns.values[1:]\n",
    "    solution = pd.concat([solution_df,df[date]],axis=1)\n",
    "    dataset = solution[date].values.T \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_global = pd.read_csv(path_time_series+ recovered_global_name)\n",
    "deaths_global = pd.read_csv(path_time_series + deaths_global_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq_rec = ['China', 'Australia', 'Denmark',\"Demark\", \"France\", 'Netherlands','United Kingdom']\n",
    "uniq_deaths = ['China', 'Australia', 'Denmark',\"Demark\", \"France\", 'Netherlands','United Kingdom','Canada']\n",
    "\n",
    "recovered_global = pre_global(recovered_global,uniq_rec)\n",
    "deaths_global = pre_global(deaths_global,uniq_deaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RECOVERED GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_rec = get_dataset_global(recovered_global) \n",
    "\n",
    "n_steps = 3\n",
    "n_features = dataset_rec.shape[1]\n",
    "model_rec = build_model_simple(n_steps,n_features)\n",
    "\n",
    "\n",
    "X_rec,y_rec = split_sequences(dataset_rec,n_steps)\n",
    "hisoty_rec = model_rec.fit(X_rec, y_rec, epochs=EPOCHS, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEATHS GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_deaths = get_dataset_global(deaths_global) \n",
    "\n",
    "n_steps = 3\n",
    "n_features = dataset_deaths.shape[1] \n",
    "model_deaths = build_model_simple(n_steps,n_features)\n",
    "X_deaths,y_deaths = split_sequences(dataset_deaths,n_steps)\n",
    "hisoty_deaths = model_deaths.fit(X_deaths, y_deaths, epochs=EPOCHS, verbose=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_global = recovered_global['Country/Region']\n",
    "last_date = pd.to_datetime(recovered_global.columns.values[-1:]).date[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictioner_rec = Prediction(model_rec,last_date,columns_global)\n",
    "df_pred_rec = predictioner_rec.predict(dataset_rec,n_features,n_steps)\n",
    "predictioner_deaths = Prediction(model_deaths,last_date,columns_global)\n",
    "df_pred_deaths = predictioner_deaths.predict(dataset_deaths,n_features,n_steps)\n",
    "submissioner = Submission(columns_global,predictioner_rec.predion_date)\n",
    "subm_global = submissioner.submit(df_pred_rec,df_pred_deaths,mapper_global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_union = pd.concat([subm_ru,subm_global],axis=0)\n",
    "solution_union = solution_union.dropna()\n",
    "solution_union.index = np.arange(solution_union.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "solution_union.prediction_confirmed = solution_union.prediction_confirmed.apply(lambda x: round(x) if x>=0 else 0)\n",
    "solution_union.prediction_deaths = solution_union.prediction_deaths.apply(lambda x: round(x) if x>=0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['RU-ALT', 'RU-AMU', 'RU-ARK', 'RU-AST', 'RU-BEL', 'RU-BRY',\n",
       "       'RU-VLA', 'RU-VGG', 'RU-VLG', 'RU-VOR', 'RU-YEV', 'RU-ZAB',\n",
       "       'RU-IVA', 'RU-IRK', 'RU-KB', 'RU-KGD', 'RU-KLU', 'RU-KAM', 'RU-KC',\n",
       "       'RU-KEM', 'RU-KIR', 'RU-KOS', 'RU-KDA', 'RU-KYA', 'RU-KGN',\n",
       "       'RU-KRS', 'RU-LEN', 'RU-LIP', 'RU-MAG', 'RU-MOW', 'RU-MOS',\n",
       "       'RU-MUR', 'RU-NEN', 'RU-NIZ', 'RU-NGR', 'RU-NVS', 'RU-OMS',\n",
       "       'RU-ORE', 'RU-ORL', 'RU-PNZ', 'RU-PER', 'RU-PRI', 'RU-PSK',\n",
       "       'RU-AD', 'RU-AL', 'RU-BA', 'RU-BU', 'RU-DA', 'RU-IN', 'RU-KL',\n",
       "       'RU-KR', 'RU-KO', 'UA-43', 'RU-ME', 'RU-MO', 'RU-SA', 'RU-SE',\n",
       "       'RU-TA', 'RU-TY', 'RU-KK', 'RU-CU', 'RU-ROS', 'RU-RYA', 'RU-SAM',\n",
       "       'RU-SPE', 'RU-SAR', 'RU-SAK', 'RU-SVE', 'UA-40', 'RU-SMO',\n",
       "       'RU-STA', 'RU-TAM', 'RU-TVE', 'RU-TOM', 'RU-TUL', 'RU-TYU',\n",
       "       'RU-UD', 'RU-ULY', 'RU-KHA', 'RU-KHM', 'RU-CHE', 'RU-CE', 'RU-CHU',\n",
       "       'RU-YAN', 'RU-YAR', 'AFG', 'ALB', 'DZA', 'AND', 'AGO', 'ATG',\n",
       "       'ARG', 'ARM', 'AUT', 'AZE', 'BHS', 'BHR', 'BGD', 'BRB', 'BLR',\n",
       "       'BEL', 'BLZ', 'BEN', 'BTN', 'BOL', 'BIH', 'BRA', 'BRN', 'BGR',\n",
       "       'BFA', 'CPV', 'KHM', 'CMR', 'CAN', 'CAF', 'TCD', 'CHL', 'COL',\n",
       "       'COG', 'COD', 'CRI', 'CIV', 'HRV', 'CUB', 'CYP', 'CZE', 'DJI',\n",
       "       'DMA', 'DOM', 'ECU', 'EGY', 'SLV', 'GNQ', 'ERI', 'EST', 'SWZ',\n",
       "       'ETH', 'FJI', 'FIN', 'GAB', 'GMB', 'GEO', 'DEU', 'GHA', 'GRD',\n",
       "       'GRC', 'GTM', 'GIN', 'GUY', 'HTI', 'VAT', 'HND', 'HUN', 'ISL',\n",
       "       'IND', 'IDN', 'IRN', 'IRQ', 'IRL', 'ISR', 'ITA', 'JAM', 'JPN',\n",
       "       'JOR', 'KAZ', 'KEN', 'KOR', 'KWT', 'KGZ', 'LVA', 'LBN', 'LBR',\n",
       "       'LBY', 'LIE', 'LTU', 'LUX', 'MDG', 'MYS', 'MDV', 'MLT', 'MRT',\n",
       "       'MUS', 'MEX', 'MDA', 'MCO', 'MNG', 'MNE', 'MAR', 'MOZ', 'NAM',\n",
       "       'NPL', 'NZL', 'NIC', 'NER', 'NGA', 'MKD', 'NOR', 'OMN', 'PAK',\n",
       "       'PAN', 'PNG', 'PRY', 'PER', 'PHL', 'POL', 'PRT', 'QAT', 'ROU',\n",
       "       'RUS', 'RWA', 'LCA', 'VCT', 'SMR', 'SAU', 'SEN', 'SRB', 'SYC',\n",
       "       'SGP', 'SVK', 'SVN', 'SOM', 'ZAF', 'ESP', 'LKA', 'SDN', 'SUR',\n",
       "       'SWE', 'CHE', 'SYR', 'TWN', 'TZA', 'THA', 'TLS', 'TGO', 'TTO',\n",
       "       'TUN', 'TUR', 'UGA', 'UKR', 'ARE', 'URY', 'USA', 'UZB', 'VEN',\n",
       "       'VNM', 'ZMB', 'ZWE', 'LAO', 'CHN', 'AUS', 'DNK', 'FRA', 'NLD',\n",
       "       'GBR'], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution_union.region.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_union.to_csv('./data/solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
